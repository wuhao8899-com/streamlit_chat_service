import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import os

# é…ç½®OpenAIå¯†é’¥
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# åˆå§‹åŒ–èŠå¤©æ¨¡å‹
@st.cache_resource
def load_chain():
    return ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.7,
        max_tokens=500
    )

llm = load_chain()

# æ„å»ºèŠå¤©ç•Œé¢
st.title("ğŸ¤– æ™ºèƒ½å®¢æœç³»ç»Ÿ")
st.caption("åŸºäº LangChain + Streamlit æ„å»º")

if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append(SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šå®¢æœåŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡å‹å¥½å›ç­”é—®é¢˜"))

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages[1:]:  # è·³è¿‡ç³»ç»Ÿæç¤º
    with st.chat_message("assistant" if msg.type == "system" else "user"):
        st.markdown(msg.content)

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)
   Â 
    # ç”Ÿæˆå›å¤
    with st.chat_message("assistant"):
        response = llm(st.session_state.messages)
        st.markdown(response.content)
        st.session_state.messages.append(response)
