import streamlit as st
import os
import sys
import re
from langchain_openai import ChatOpenAI
# è°ƒç”¨æ–¹å¼ï¼ˆä½¿ç”¨ LCEL è¯­æ³•æˆ–ç›´æ¥æ„é€  messagesï¼‰
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from embedding.zhipuai_embedding import ZhipuAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from dotenv import load_dotenv, find_dotenv
from langchain.chains import ConversationalRetrievalChain
from memory_module import memory

_ = load_dotenv(find_dotenv())
st.title('ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘')
#è¾“å…¥å¯†ç 
api_key = st.sidebar.text_input('OpenAI API Key', type ='password')
# å†å²èŠå¤©è®°å½•å­˜å‚¨
if 'history' not in st.session_state:
    st.session_state['history'] = []
# langchainè‡ªåŠ¨çš„å­˜å‚¨
if 'memory' not in st.session_state:
    st.session_state['memory'] = memory
# é€‰æ‹©å“ªç§åº”ç­”çš„æ¨¡å¼
selected_method = st.radio(
        "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
        ["None", "qa_chain", "chat_qa_chain"],
        captions 
= ["ä¸ä½¿ç”¨æ£€ç´¢é—®ç­”çš„æ™®é€šæ¨¡å¼", "ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼", "å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼"])
# è·å¾—å¤§è¯­è¨€æ¨¡å‹
# def getllm(fun):
#     try:
#         llm = ChatOpenAI(
#             openai_api_key = api_key,  # ä½ çš„å¯†é’¥
#             openai_api_base ="https://api.gptsapi.net/v1",  # è‡ªå®šä¹‰ API åœ°å€
#             model_name ='gpt-4-turbo',  # æ ¹æ®ä½ çš„æœåŠ¡å•†æ”¯æŒçš„æ¨¡å‹åç§°è°ƒæ•´
#             temperature = 0
#         )

#     except Exception as e:
#         st.warning("æ£€æŸ¥ä½ çš„api_key")
# ç”¨äºæ˜¾ç¤º
def showStr():
    for data in  st.session_state.history:
        str1 = data["user"]
        messages.chat_message("user").write(str1)
        str2 = data["assistant"]
        messages.chat_message("assistant").write(str2)
# è·å¾—å‘é‡çš„æ•°æ®åº“
def getVectordb():
    # åŠ è½½å‘é‡æ•°æ®åº“
    embedding = ZhipuAIEmbeddings()
    persist_directory = "D:\\code_project/knowledge_db/chroma_db" # æ•°æ®åº“å­˜å‚¨è·¯å¾„
    # å‡†å¤‡å¥½å‘é‡çš„æ•°æ®åº“
    vectordb = Chroma(
        embedding_function=embedding,
        persist_directory=persist_directory  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
    )
    return vectordb
# å•æ¬¡çš„é—®ç­”é“¾,å¸¦æ£€ç´¢
def getAskByTemple(text, llm):
    vectordb = getVectordb()
    # è¿™é‡Œçš„contextæˆ‘ä»¬ä¸éœ€è¦ç®¡ï¼Œlanchainä¼šå»æ•°æ®åº“ç´¢å¼•åˆ°ç›¸å…³çš„contextå¡«å…¥
    template = """1ã€ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
    æ¡ˆã€‚ä½ åº”è¯¥ä½¿ç­”æ¡ˆå°½å¯èƒ½è¯¦ç»†å…·ä½“ï¼Œä½†ä¸è¦åé¢˜ã€‚å¦‚æœç­”æ¡ˆæ¯”è¾ƒé•¿ï¼Œè¯·é…Œæƒ…è¿›è¡Œåˆ†æ®µï¼Œä»¥æé«˜ç­”æ¡ˆçš„é˜…è¯»ä½“éªŒã€‚
    å¦‚æœç­”æ¡ˆæœ‰å‡ ç‚¹ï¼Œä½ åº”è¯¥åˆ†ç‚¹æ ‡å·å›ç­”ï¼Œè®©ç­”æ¡ˆæ¸…æ™°å…·ä½“ã€‚
    è¯·ä½ é™„ä¸Šå›ç­”çš„æ¥æºåŸæ–‡ï¼Œä»¥ä¿è¯å›ç­”çš„æ­£ç¡®æ€§ã€‚
    {context}
    é—®é¢˜: {question}
    æœ‰ç”¨çš„å›ç­”:
    2ã€åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼Œåæ€å›ç­”ä¸­æœ‰æ²¡æœ‰ä¸æ­£ç¡®æˆ–ä¸æ˜¯åŸºäºä¸Šä¸‹æ–‡å¾—åˆ°çš„å†…å®¹ï¼Œå¦‚æœæœ‰ï¼Œå›ç­”ä½ ä¸çŸ¥é“
    ç¡®ä¿ä½ æ‰§è¡Œäº†æ¯ä¸€ä¸ªæ­¥éª¤ï¼Œä¸è¦è·³è¿‡ä»»æ„ä¸€ä¸ªæ­¥éª¤ã€‚
    """
    QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context","question"],
                    template=template)
    # æ•°æ®åº“çš„æ•°æ®æ¥æº
    retriever= vectordb.as_retriever()
    #è¿™æ˜¯ä¸€ä¸ªé—®ç­”é“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever = retriever ,
        return_source_documents= True,
        chain_type_kwargs={"prompt":QA_CHAIN_PROMPT})
    result = qa_chain({"query": text})
    temp = {}
    temp["user"] = text
    temp["assistant"] = result['result']
    st.session_state.history.append(temp)
    showStr()
# ç›´æ¥çš„é—®ç­”
def getResponseByNone(text, llm):
    response = llm.invoke(
        [SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å›ç­”åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©å›ç­”é—®é¢˜'),
        HumanMessage(content=text)
    ])
    output_parser = StrOutputParser()
    str = output_parser.invoke(response)
    temp = {}
    temp["user"] = text
    temp["assistant"] = str
    st.session_state.history.append(temp)
    showStr()
# ç›´æ¥çš„é—®ç­”
def getResponseByChain_memory(text, llm):
    vectordb = getVectordb()
    # æ•°æ®åº“çš„æ•°æ®æ¥æº
    retriever= vectordb.as_retriever()
    qa = ConversationalRetrievalChain.from_llm(
        llm,
        retriever= retriever,
        memory= st.session_state.memory)
    result = qa({"question": text})
    temp = {}
    temp["user"] = text
    temp["assistant"] = result["answer"]
    st.session_state.history.append(temp)
    showStr()
# å¯¹è¯æ¶ˆæ¯æ¡†
messages = st.container(height=300)
prompt = st.chat_input("Say something", key="user_input")

callback_method = {
    "None":getResponseByNone,
    "qa_chain":getAskByTemple,
    "chat_qa_chain":getResponseByChain_memory,
}
# è¾“å‡ºå­—ç¬¦ä¸²
def get_completion(prompt):
    prompt_input = '''
    è¯·åˆ¤æ–­ä»¥ä¸‹é—®é¢˜ä¸­æ˜¯å¦åŒ…å«å¯¹è¾“å‡ºçš„æ ¼å¼è¦æ±‚ï¼Œå¹¶æŒ‰ä»¥ä¸‹è¦æ±‚è¾“å‡ºï¼š
    è¯·è¿”å›ç»™æˆ‘ä¸€ä¸ªå¯è§£æçš„Pythonåˆ—è¡¨ï¼Œåˆ—è¡¨ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¯¹è¾“å‡ºçš„æ ¼å¼è¦æ±‚ï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªæŒ‡ä»¤ï¼›ç¬¬äºŒä¸ªå…ƒç´ æ˜¯å»æ‰æ ¼å¼è¦æ±‚çš„é—®é¢˜åŸæ–‡
    å¦‚æœæ²¡æœ‰æ ¼å¼è¦æ±‚ï¼Œè¯·å°†ç¬¬ä¸€ä¸ªå…ƒç´ ç½®ä¸ºç©º
    éœ€è¦åˆ¤æ–­çš„é—®é¢˜ï¼š
    ~~~
    {}
    ~~~
    ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹æˆ–æ ¼å¼ï¼Œç¡®ä¿è¿”å›ç»“æœå¯è§£æã€‚
    '''
    prompt_input = prompt_input.format(prompt)
    llm = ChatOpenAI(
        openai_api_key="sk-NNT0fd5cc4814d18b0c81bab0fd838b7ea542466119qPmKM",  # ä½ çš„å¯†é’¥
        openai_api_base="https://api.gptsapi.net/v1",  # è‡ªå®šä¹‰ API åœ°å€
        model_name='gpt-4-turbo'  # æ ¹æ®ä½ çš„æœåŠ¡å•†æ”¯æŒçš„æ¨¡å‹åç§°è°ƒæ•´
    )
    response = llm.invoke(
        [SystemMessage(content='ä½ æ˜¯ä¸€åå¤§æ¨¡å‹çŸ¥è¯†çš„æ™ºèƒ½å›ç­”åŠ©æ‰‹'),
        HumanMessage(content=prompt_input)
    ])
    output_parser = StrOutputParser()
    aaa = output_parser.invoke(response)
    return aaa

# input_lst_s = get_completion("LLMçš„åˆ†ç±»æ˜¯ä»€ä¹ˆï¼Ÿç»™æˆ‘è¿”å›ä¸€ä¸ª Python List")
# # æ‰¾åˆ°æ‹†åˆ†ä¹‹ååˆ—è¡¨çš„èµ·å§‹å’Œç»“æŸå­—ç¬¦
# start_loc = input_lst_s.find('[')
# end_loc = input_lst_s.find(']')
# rule, new_question = eval(input_lst_s[start_loc:end_loc+1])
if prompt:
    if not api_key:
        st.warning('Please enter your OpenAI API key!', icon='âš ')
    elif not api_key.startswith('sk-'):
        st.warning('æ£€æŸ¥ä½ çš„æ‹¼å†™', icon='âš ')
    else:
        try:
            llm = ChatOpenAI(
                openai_api_key = api_key,  # ä½ çš„å¯†é’¥
                openai_api_base ="https://api.gptsapi.net/v1",  # è‡ªå®šä¹‰ API åœ°å€
                model_name ='gpt-4-turbo',  # æ ¹æ®ä½ çš„æœåŠ¡å•†æ”¯æŒçš„æ¨¡å‹åç§°è°ƒæ•´
                temperature = 0
            )
            test_prompt = "Hello, how are you?"
            response = llm.invoke(test_prompt)
            if response:
                print("llm åˆå§‹åŒ–æˆåŠŸå¹¶å¯ä»¥æ­£å¸¸å·¥ä½œï¼")
                callback_method.get(selected_method, getAskByTemple)(prompt, llm)
            else:
                print("æ£€æŸ¥ä½ çš„apiå»")
        except Exception as e:
            st.warning("æ£€æŸ¥ä½ çš„api_key")
        